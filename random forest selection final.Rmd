---
title: "random forest feature selection"
author: "fatima"
date: "13/07/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



```{r }
library(corrplot)
library(dplyr)
library(caret)
library(e1071)
library(ROSE)
library(randomForest)
data.feature.selection <- read.csv("C:/Users/fatim/Downloads/capstone initial results/data.rf.selection.csv")
str(data.feature.selection)
```

```{r}
colnames(data.feature.selection)
```
##removing lead.time,country,agent,last.reservation.status,last.reservation.status.date,total.guests,adr,total.nights,grp.total.nights and first column
```{r}
data.randomforest<-data.feature.selection[-c(1,2,13,23,26,29,30,32,33,38)]
```

## transforming
```{r}
data.randomforest<-transform(data.randomforest,arrival.year = factor(arrival.year),repeated.guest = factor(repeated.guest),    is.cancelled = factor(is.cancelled))
```

## correlation

```{r}
## changing factor to int to find correlation


fac.col= data.randomforest %>% Filter(f=is.factor) %>% colnames()
data.randomforest.copy<-data.randomforest
data.randomforest.copy[,fac.col] = data.frame(sapply( data.randomforest.copy[,fac.col], as.integer))
str(data.randomforest.copy)
```



```{r}
d.cor<-cor(data.randomforest.copy,method="spearman")

corrplot(d.cor,method = "color")
```

##It is better to remove  highly correlated variables when using random forest for fetaure selection
##random forest is robust to outliers so no need to treat them
##removing last.reservation.status.year,total.weekend.nights,distribution channel,repeated guest,assigned.room.type
```{r}
data.randomforest<-data.randomforest[-c(26,5,12,13,17)]
```


```{r}
xtabs(~reserved.room.type+is.cancelled,data.randomforest)

```

##dropping levels B and L from reserved room type otherwise it might create problem while sampling because of very few observations
```{r}
data.randomforest<-filter(data.randomforest,reserved.room.type!="B" & reserved.room.type!="L")
data.randomforest$reserved.room.type<-droplevels(data.randomforest$reserved.room.type)
```

## feature selection
```{r}
feature.sel<-randomForest(is.cancelled~.,data=data.randomforest)
varImpPlot(feature.sel)
```
## taking variables with meanDecreaseGini >200

```{r}
model.var<-c("grp.country","grp.lead.time","market.segment","required.car.spaces","arrival.day.of.month","arrival.week.of.year",
"total.week.nights","arrival.month","no.of.special.requests","arrival.year","reserved.room.type","grp.adr","no.of.booking.changes","customer.type","is.cancelled")
```

## data for modelling
```{r}
mod.data<-data.randomforest[model.var]

```

## splitting dataset into training and testing 

```{r}
set.seed(101)
row.no.train<-sample(1:nrow(mod.data),nrow(mod.data)*0.8)
train.hotel<-mod.data[row.no.train,]
test.hotel<-mod.data[-row.no.train,]
```

# removing target variable from test data

```{r}
test.hotel.new<-test.hotel[-15]
```

## balancing dataset(only training set)

```{r}
over.data<- ovun.sample(is.cancelled~.,data=train.hotel,method="over",N=40732)$data
under.data<-ovun.sample(is.cancelled~.,data=train.hotel,method="under",N=12468)$data
```

## random forest
```{r}
model.rf1.tree<-randomForest(is.cancelled ~ .,data=train.hotel)
predict.rf1<-predict(model.rf1.tree,test.hotel.new)
confusionMatrix(predict.rf1,test.hotel$is.cancelled,positive="1")
```

## oversampling
```{r}
model.rf1.over.balanced<-randomForest(is.cancelled ~ .,data=over.data)
confusionMatrix(predict(model.rf1.over.balanced,test.hotel.new),test.hotel$is.cancelled,positive="1")
```
## undersampling

```{r}
model.rf1.under.balanced<-randomForest(is.cancelled ~ .,data=under.data)
confusionMatrix(predict(model.rf1.under.balanced,test.hotel.new),test.hotel$is.cancelled,positive="1")
```
## Random forest with imbalanced dataset-> accuracy is 86.15%
## After oversampling -> accuracy is 85.51 %
## after undersampling -> accuracy is 82.45 %


## decision tree
```{r}
library(rpart)
library(rpart.plot)
```


```{r}
tree.model1<-rpart(is.cancelled~.,method='class',parms=list(split='information'),data=train.hotel) ## method class for classification tree
printcp(tree.model1)
```

```{r}
## growing full tree for this specify cp=-1..might create overfitted but can be pruned later

tree.model2<-rpart(is.cancelled~.,method='class',parms=list(split='information'),data=train.hotel,cp=-1)
printcp(tree.model2)
tree.model2$variable.importance

```
## pruning 

```{r}
tree.model3<-rpart(is.cancelled~.,method='class',parms=list(split='information'),data=train.hotel,cp=0.00075)
```

## confusion matrix for decision tree

```{r}
predicted.tree.model1<-predict(tree.model1,test.hotel.new,type="class")
predicted.tree.model2<-predict(tree.model2,test.hotel.new,type="class")
predicted.tree.model3<-predict(tree.model3,test.hotel.new,type="class")
c.matrix1<-table(test.hotel$is.cancelled,predicted.tree.model1)
c.matrix2<-table(test.hotel$is.cancelled,predicted.tree.model2)
c.matrix3<-table(test.hotel$is.cancelled,predicted.tree.model3)
```

## accuracy

```{r}
sum(diag(c.matrix1))/nrow(test.hotel)
sum(diag(c.matrix2))/nrow(test.hotel)
sum(diag(c.matrix3))/nrow(test.hotel)
```

## oversampling

```{r}
set.seed(123)
tree.over1<-rpart(is.cancelled~.,method='class',data=over.data,parms=list(split='information'))
tree.over2<-rpart(is.cancelled~.,method='class',data=over.data,parms=list(split='information'),cp=-1)
printcp(tree.over2)
tree.over3<-rpart(is.cancelled~.,method='class',data=over.data,parms=list(split='information'),cp=0.000064)
```

```{r}
predicted.tree.over1<-predict(tree.over1,test.hotel.new,type="class")
predicted.tree.over2<-predict(tree.over2,test.hotel.new,type="class")
predicted.tree.over3<-predict(tree.over3,test.hotel.new,type="class")
matrix.over1<-table(test.hotel$is.cancelled,predicted.tree.over1)
matrix.over2<-table(test.hotel$is.cancelled,predicted.tree.over2)
matrix.over3<-table(test.hotel$is.cancelled,predicted.tree.over3)
```

```{r}
sum(diag(matrix.over1))/nrow(test.hotel)
sum(diag(matrix.over2))/nrow(test.hotel)
sum(diag(matrix.over3))/nrow(test.hotel)
```
## undersampling
```{r}
set.seed(121)
tree.under1<-rpart(is.cancelled~.,method='class',data=under.data,parms=list(split='information'))
tree.under2<-rpart(is.cancelled~.,method='class',data=under.data,parms=list(split='information'),cp=-1)
printcp(tree.under2)
tree.under3<-rpart(is.cancelled~.,method='class',data=under.data,parms=list(split='information'),cp=0.00105)
```
## prediction
```{r}
predicted.tree.under1<-predict(tree.under1,test.hotel.new,type="class")
predicted.tree.under2<-predict(tree.under2,test.hotel.new,type="class")
predicted.tree.under3<-predict(tree.under3,test.hotel.new,type="class")
matrix.under1<-table(test.hotel$is.cancelled,predicted.tree.under1)
matrix.under2<-table(test.hotel$is.cancelled,predicted.tree.under2)
matrix.under3<-table(test.hotel$is.cancelled,predicted.tree.under3)
```
## accuracy
```{r}
sum(diag(matrix.under1))/nrow(test.hotel)
sum(diag(matrix.under2))/nrow(test.hotel)
sum(diag(matrix.under3))/nrow(test.hotel)
```
## best accuracy for decision tree is same for oversampling and undersampling 81.80%

## logistic regression
##for logistic regression we should treat outliers and should normalise



```{r}
df.copy<-mod.data
num.names<-df.copy%>% Filter(f=is.integer) %>% colnames()
num.names
```
## removing outliers from total.week.nights
```{r}
df.copy<-filter(df.copy,total.week.nights<12)
```
## grouping required car spaces
```{r}
group_required.car.spaces<-function(num){
  
  if(num==0){
    return('0')
  }else{
    return('more than 0')
  }
}

df.copy$required.car.spaces<-sapply(df.copy$required.car.spaces,group_required.car.spaces)
df.copy$required.car.spaces<-factor(df.copy$required.car.spaces)
levels(df.copy$required.car.spaces)
```
## grouping no. of special requests
```{r}
group_no.of.special.requests<-function(num){
  
  if(num==0){
    return('0')
  }else if(num==1){
    return('1')
  }else if(num==2){
    return('2')
  }else{
    return('other')
  }
    
}
  
df.copy$no.of.special.requests<-sapply(df.copy$no.of.special.requests,group_no.of.special.requests)
df.copy$no.of.special.requests<-factor(df.copy$no.of.special.requests)
levels(df.copy$no.of.special.requests)  

```
## grouping no. of booking changes
```{r}
group_no.of.booking.changes<-function(num){
  if(num==0){
    return('0')
  }else{
    return('other')
  }
}

df.copy$no.of.booking.changes<-sapply(df.copy$no.of.booking.changes,group_no.of.booking.changes)
df.copy$no.of.booking.changes<-factor(df.copy$no.of.booking.changes)
levels(df.copy$no.of.booking.changes)  
```

## Normalising numeric variables


```{r}
fun<- function(x){
  x<- (x-min(x))/(max(x)-min(x))
  return(x)
}
n.names<-df.copy %>% Filter(f=is.integer) %>% colnames()
n.names
df.copy[n.names]<-sapply(df.copy[,n.names],fun)
head(df.copy)
```

```{r}
set.seed(101)
row.train<-sample(1:nrow(df.copy),nrow(df.copy)*0.8)
train<-df.copy[row.train,]
test<-df.copy[-row.train,]
```
# removing target variable from test data
```{r}
test.new<-test[-15]
```

## logistic regression without balanacing

```{r}
glm.model<-glm(is.cancelled ~ .,family="binomial",data=train)
summary(glm.model)
```
## predicting 
```{r}
predicted.prob <- predict(glm.model, test.new, type="response")
predicted_values<- ifelse(predicted.prob>=0.5, 1, 0)
```

## confusion matrix and accuracy
```{r}
confusion.matrix <- table(actual = test$is.cancelled, predicted = predicted_values)
sum(diag(confusion.matrix))/nrow(test)## this is accuracy
confusion.matrix
```
## balancing dataset(only training set)

```{r}
over1<- ovun.sample(is.cancelled~.,data=train,method="over",N=40466)$data
under1<-ovun.sample(is.cancelled~.,data=train,method="under",N=12454)$data
```
## building model(oversampling)
```{r}
glm.model.over.balanced<-glm(is.cancelled ~ .,family="binomial",data=over1)
summary(glm.model.over.balanced)

predicted.prob1 <- predict(glm.model.over.balanced, test.new, type="response")
predicted_values1<- ifelse(predicted.prob1>=0.5, 1, 0)

confusion.matrix1 <- table(actual = test$is.cancelled, predicted = predicted_values1)
sum(diag(confusion.matrix1))/nrow(test)## this is accuracy
confusion.matrix1
```
## undersampling
```{r}
glm.model.under.balanced<-glm(is.cancelled ~ .,family="binomial",data=under1)
summary(glm.model.under.balanced)
predicted.prob2 <- predict(glm.model.under.balanced, test.new, type="response")
predicted_values2<- ifelse(predicted.prob2>=0.5, 1, 0)

confusion.matrix2 <- table(actual = test$is.cancelled, predicted = predicted_values2)
sum(diag(confusion.matrix2))/nrow(test)## this is accuracy
confusion.matrix2

```
## for logistic regression with imbalanced dataset-> accuracy is 83.4%
## for logistic regression with undersampling-> accuracy is 78.8%
## for logistic regression with oversampling-> accuracy is 78.9%


## Naive bayes
## naive bayes performs better when input variables are independent and categorical and assumes numeric variables as normal
## imbalanced
```{r}
model.nb<-naiveBayes(is.cancelled~.,data=train)
predict.nb<-predict(model.nb,test.new)
confusion.matrix.nb<-table(actual=test$is.cancelled,predicted=predict.nb)
confusion.matrix.nb
sum(diag(confusion.matrix.nb))/nrow(test)
```
## balanced and oversampled

```{r}
model.nb.over<-naiveBayes(is.cancelled~.,data=over1)
predict.nb.over<-predict(model.nb.over,test.new)
confusion.matrix.nb.over<-table(actual=test$is.cancelled,predicted=predict.nb.over)
confusion.matrix.nb.over
sum(diag(confusion.matrix.nb.over))/nrow(test)
```
## balanced and undersampled
```{r}
model.nb.under<-naiveBayes(is.cancelled~.,data=under1)
predict.nb.under<-predict(model.nb.under,test.new)
confusion.matrix.nb.under<-table(actual=test$is.cancelled,predicted=predict.nb.under)
confusion.matrix.nb.under
sum(diag(confusion.matrix.nb.under))/nrow(test)
```
## Naive bayes accuracy for oversample 74.8% and for undersample is 74.6%


